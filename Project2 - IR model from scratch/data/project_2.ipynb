{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import math\n",
    "from collections import Counter\n",
    "import flask\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "import json\n",
    "import time\n",
    "import ast\n",
    "\n",
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def removeStopwords(tokens):\n",
    "    filtered_sentence = []\n",
    "    for w in tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    return filtered_sentence\n",
    "\n",
    "#Porter Stemming\n",
    "ps = PorterStemmer()\n",
    "def stemming(tokens):\n",
    "    stemmed_tokens = []\n",
    "    for w in tokens:\n",
    "        stemmed_tokens.append(ps.stem(w))\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Creating a Linked List Node\n",
    "class Node:\n",
    "   def __init__(self, data=None, tf_idf = 0):\n",
    "      self.data = data\n",
    "      self.skip = None\n",
    "      self.next = None\n",
    "      self.tf_idf = tf_idf\n",
    "\n",
    "# Inserting into Linked List\n",
    "def insertAtEnd(head, doc_id, tf_idf):\n",
    "    tempHead = Node(doc_id, tf_idf)\n",
    "      \n",
    "    if (head == None):\n",
    "        head = tempHead\n",
    "    else:\n",
    "        cur_docid = head\n",
    "        while (cur_docid.next != None):\n",
    "            cur_docid = cur_docid.next\n",
    "        cur_docid.next = tempHead\n",
    "      \n",
    "    return head\n",
    "\n",
    "# Printing a linked list\n",
    "def printLinkedList(head):\n",
    "    cur = head\n",
    "    while (cur != None) :\n",
    "        print(\"val\", cur.data, \"tf_idf\",cur.tf_idf,  end = \" \")\n",
    "        # print(\"val\", cur.data)\n",
    "        cur = cur.next\n",
    "\n",
    "# Length of a linked list\n",
    "def LinkedListLength(head):\n",
    "    length = 0\n",
    "    cur = head\n",
    "    while (cur != None) :\n",
    "        length += 1\n",
    "        cur = cur.next\n",
    "    \n",
    "    return length\n",
    "\n",
    "# Copy a Linked List\n",
    "def copy_list(head):\n",
    "    cur = head\n",
    "    if cur is None:\n",
    "        return cur\n",
    "    \n",
    "    # Create a new node\n",
    "    new_node = Node(cur.data, cur.tf_idf)\n",
    "    # The new nodes next pointer would point to the node returned from recursion\n",
    "    # i.e. the next new node.\n",
    "    new_node.next = copy_list(cur.next)\n",
    "    \n",
    "    return new_node\n",
    "\n",
    "# Removing Duplicates\n",
    "def removeDuplicates(list):\n",
    "    cur = list\n",
    "    while cur and cur.next:\n",
    "        nxt = cur.next\n",
    "        if cur.data == nxt.data:\n",
    "            cur.next = nxt.next\n",
    "        else:\n",
    "            cur = nxt\n",
    "    \n",
    "    return list\n",
    "\n",
    "# Merge 2 Linked Lists\n",
    "def merge_list(Linkedlist1, Linkedlist2):\n",
    "    newnode = Node()\n",
    "    tail = newnode\n",
    "    newnode.next = None\n",
    "    comparisions = 0\n",
    "\n",
    "    list1 = Linkedlist1\n",
    "    list2 = Linkedlist2\n",
    "    while list1 != None and list2 != None:\n",
    "        if list1.data == list2.data:\n",
    "            comparisions += 1\n",
    "            tail.next = push((tail.next), list1.data, max(list1.tf_idf, list2.tf_idf))\n",
    "            tail = tail.next\n",
    "            list1 = list1.next\n",
    "            list2 = list2.next\n",
    "        else:\n",
    "            if list1.data > list2.data:\n",
    "                comparisions += 1\n",
    "                list2 = list2.next\n",
    "            else:\n",
    "                comparisions += 1\n",
    "                list1 = list1.next\n",
    "    \n",
    "    return newnode.next, comparisions\n",
    "\n",
    "# Merge 2 Linked Lists Skip\n",
    "def merge_listSkip(Linkedlist1, Linkedlist2):\n",
    "    newnode = Node()\n",
    "    newnode.next = None\n",
    "    res_head = newnode\n",
    "    tail = newnode\n",
    "    \n",
    "    comparisions = 0\n",
    "\n",
    "    list1 = Linkedlist1\n",
    "    list2 = Linkedlist2\n",
    "\n",
    "    while list1 != None and list2 != None:\n",
    "        if list1.data == list2.data:\n",
    "            comparisions += 1\n",
    "            tail.next = push((tail.next), list1.data, max(list1.tf_idf, list2.tf_idf))\n",
    "            tail = tail.next\n",
    "            list1 = list1.next\n",
    "            list2 = list2.next\n",
    "        else:\n",
    "            if list1.data > list2.data:\n",
    "                if list2.skip and list2.skip != list2 and list1.data > list2.skip.data:\n",
    "                    while list1 and list2 and list2.skip and list1.data >= list2.skip.data:\n",
    "                        comparisions += 1\n",
    "                        list2 = list2.skip\n",
    "                else:\n",
    "                    comparisions += 1\n",
    "                    list2 = list2.next\n",
    "            else:\n",
    "                if list1.skip and list1.skip != list1 and list2.data > list1.skip.data:\n",
    "                    while list1 and list2 and list1.skip and (list2.data >= list1.skip.data):\n",
    "                        comparisions += 1\n",
    "                        list1 = list1.skip\n",
    "                else:\n",
    "                    comparisions += 1\n",
    "                    list1 = list1.next\n",
    "    \n",
    "    return newnode.next, comparisions\n",
    "\n",
    "\n",
    "# merge n linked lists\n",
    "def merge_lists(postings_list, skip):\n",
    "    sorted_lists = sorted(postings_list, key=lambda k: LinkedListLength(postings_list[k]), reverse=False)\n",
    "    # print(sorted_lists[0])\n",
    "\n",
    "    list1 = postings_list[sorted_lists[0]]\n",
    "\n",
    "    total_comparisions = 0\n",
    "    for i in range(1, len(sorted_lists)):\n",
    "\n",
    "        list2 = postings_list[sorted_lists[i]]\n",
    "\n",
    "        if skip:\n",
    "            list1, comparisions = merge_listSkip(list1, list2)\n",
    "        else:\n",
    "            list1, comparisions = merge_list(list1, list2)\n",
    "\n",
    "        total_comparisions += comparisions\n",
    "\n",
    "    return list1, total_comparisions\n",
    "    \n",
    "\n",
    "\n",
    "def push(head_ref, new_data, tf_idf):\n",
    "    ''' allocate node '''\n",
    "    new_node = Node()\n",
    "  \n",
    "    ''' put in the data  '''\n",
    "    new_node.data = new_data\n",
    "    new_node.tf_idf = tf_idf\n",
    "  \n",
    "    ''' link the old list off the new node '''\n",
    "    new_node.next = (head_ref)\n",
    "  \n",
    "    ''' move the head to point to the new node '''\n",
    "    (head_ref) = new_node;   \n",
    "    return head_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readInputCorpus(file):\n",
    "# Read input data\n",
    "    input_corpus = pd.read_csv(file, delimiter = '\\t', header=None)\n",
    "    # Renaming columns\n",
    "    input_corpus = input_corpus.rename(columns={0: \"document_id\", 1: \"body\"})\n",
    "\n",
    "    # print(input_corpus.head())\n",
    "    return input_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lowecase\n",
    "def convertLower(input_corpus):\n",
    "    input_corpus[\"body\"] = input_corpus[\"body\"].str.lower()\n",
    "    return input_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre-Processing\n",
    "# 1. Removed special characters\n",
    "# 2. Removed excess white spaces\n",
    "# 3. Whitespace tokenization\n",
    "# 4. Removed stopwords\n",
    "# 5. Performed Porter Stemming\n",
    "def inputCorpusPreprocessing(input_corpus):\n",
    "    input_corpus_prosessed = input_corpus.copy()\n",
    "    for i in range(len(input_corpus_prosessed)):\n",
    "        removed_text = re.sub(r\"[^a-zA-Z0-9 ]\", ' ', input_corpus_prosessed['body'][i])\n",
    "        # removed_text = re.sub('[\\W_]+', ' ', input_corpus_prosessed['body'][i])\n",
    "        removing_extra_spaces = re.sub('\\s{2,}', ' ', removed_text)\n",
    "        stripped = removing_extra_spaces.strip()\n",
    "        tokens = re.split('\\s+', stripped)\n",
    "        removed_stopwords = removeStopwords(tokens)\n",
    "        stemmed_tokens = stemming(removed_stopwords)\n",
    "        input_corpus_prosessed['body'][i] = stemmed_tokens\n",
    "        \n",
    "    return input_corpus_prosessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Postings lists\n",
    "def CreateArrayPostingsLists(input_corpus_prosessed):\n",
    "    postings_lists = {}\n",
    "\n",
    "    for i in range(len(input_corpus_prosessed)):\n",
    "        counter = Counter(input_corpus_prosessed['body'][i])\n",
    "        length = len(input_corpus_prosessed['body'][i])\n",
    "        for j in input_corpus_prosessed['body'][i]:\n",
    "            tf = counter[j]/length\n",
    "            # print(tf)\n",
    "            if j in postings_lists:\n",
    "                postings_lists[j].append([input_corpus_prosessed['document_id'][i], tf])\n",
    "            else:\n",
    "                postings_lists[j] = [[input_corpus_prosessed['document_id'][i], tf]]\n",
    "    \n",
    "\n",
    "    # Sorting Postings lists in increasing order\n",
    "    for i in postings_lists:\n",
    "        postings_lists[i].sort()\n",
    "\n",
    "    return postings_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_corpus = readInputCorpus('input_corpus.txt') #Reading Corpus\n",
    "# input_corpus = convertLower(input_corpus) #LowerCase\n",
    "# input_corpus_prosessed = inputCorpusPreprocessing(input_corpus) #Data Pre-processing\n",
    "# postings_lists = CreateArrayPostingsLists(input_corpus_prosessed) #Array Postings Lists\n",
    "# postings_linkedLists = CovertListToLinkedList(postings_lists, input_corpus_prosessed) #Converts Array Lists to Linked Lists\n",
    "# # # InputCourpusValidityChecks(postings_linkedLists, postings_lists) #Validity Checks\n",
    "# skip_postings_lists = SkipPostingsLists(postings_linkedLists) #Skip Postings List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array postings lists to linked list\n",
    "def CovertListToLinkedList(postings_lists, input_corpus_prosessed):\n",
    "    postings_linkedLists = {}\n",
    "\n",
    "    docs_length = len(input_corpus_prosessed)\n",
    "    # print(postings_lists)\n",
    "    for i in postings_lists:\n",
    "        newnode = None\n",
    "        arr = []\n",
    "        for k in postings_lists[i]:\n",
    "            arr.append(k[0])\n",
    "        \n",
    "        idf_length = len(set(arr))\n",
    "        for j in postings_lists[i]:\n",
    "\n",
    "            tf_idf = (docs_length/idf_length) * j[1]\n",
    "            newnode = insertAtEnd(newnode, j[0], tf_idf)\n",
    "        postings_linkedLists[i] = newnode\n",
    "\n",
    "    return postings_linkedLists\n",
    "\n",
    "    # printLinkedList(postings_linkedLists['recent'])\n",
    "    # LinkedListLength(postings_linkedLists['recent'])\n",
    "\n",
    "    # return postings_linkedLists, postings_linkedLists_TfIDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validity Checks\n",
    "# Printing Lengths of each term postings linked list\n",
    "def InputCourpusValidityChecks(postings_linkedLists, postings_lists):\n",
    "    ll_lengths = []\n",
    "    for i in postings_linkedLists:\n",
    "        ll_lengths.append(LinkedListLength(postings_linkedLists[i]))\n",
    "\n",
    "    # print(ll_lengths[:20])\n",
    "\n",
    "        # Printing Lengths of each term postings list\n",
    "    ar_lengths = []\n",
    "    for i in postings_lists:\n",
    "        ar_lengths.append(len(postings_lists[i]))\n",
    "\n",
    "    for i in range(len(ll_lengths)):\n",
    "        if ll_lengths[i] != ar_lengths[i]:\n",
    "            print('False')\n",
    "\n",
    "    skip_postings_lists = postings_linkedLists.copy()\n",
    "\n",
    "    head = skip_postings_lists['recent']\n",
    "\n",
    "    while head != None:\n",
    "        if head.skip:\n",
    "            print(head.data, head.skip.data, head.tf_idf, head.skip.tf_idf)\n",
    "        else:\n",
    "            print(head.data, head.skip, head.tf_idf)\n",
    "\n",
    "        head = head.next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postings Lists with skip pointers\n",
    "def SkipPostingsLists(postings_linkedLists):\n",
    "\n",
    "\n",
    "    skip_postings_lists = postings_linkedLists.copy()\n",
    "        \n",
    "    for i in skip_postings_lists:\n",
    "        skip_postings_lists[i] = removeDuplicates(skip_postings_lists[i])\n",
    "        postings_length = LinkedListLength(skip_postings_lists[i])\n",
    "\n",
    "        if (math.isqrt(postings_length) ** 2 == postings_length):\n",
    "            # skip_length = round(math.sqrt(postings_length))\n",
    "            skip_length = math.floor(math.sqrt(postings_length)) - 1\n",
    "        else:\n",
    "            # skip_length = round(math.sqrt(postings_length))\n",
    "            skip_length = math.floor(math.sqrt(postings_length))\n",
    "\n",
    "        # if i == 'coronaviru':\n",
    "        #     print(LinkedListLength(skip_postings_lists[i]), skip_length)\n",
    "\n",
    "        head = skip_postings_lists[i]\n",
    "        cur = head\n",
    "        temp = None\n",
    "        if (skip_length == 0) or (skip_length == 1):\n",
    "            continue\n",
    "        \n",
    "        while cur != None:\n",
    "            temp = cur\n",
    "            last = None\n",
    "            \n",
    "            for k in range(skip_length+1):\n",
    "                if cur != None:               \n",
    "                    last = cur\n",
    "                    cur = cur.next\n",
    "\n",
    "            if (cur == None):\n",
    "                temp.skip = last\n",
    "            else:    \n",
    "                temp.skip = cur\n",
    "    \n",
    "\n",
    "    return skip_postings_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur = skip_postings_lists['coronaviru']\n",
    "# while cur != None:\n",
    "#     if cur.skip:\n",
    "#         print(cur.data, cur.skip.data)\n",
    "#     else:\n",
    "#         print(cur.data)\n",
    "    \n",
    "#     cur = cur.next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Pointers Validity Check\n",
    "def skipValidityCheck(postings_linkedLists):\n",
    "    head = postings_linkedLists['recent']\n",
    "\n",
    "    while head != None:\n",
    "        if head.skip:\n",
    "            print(head.data, head.skip.data, head.tf_idf, head.skip.tf_idf)\n",
    "        else:\n",
    "            print(head.data, head.skip, head.tf_idf)\n",
    "\n",
    "        head = head.next\n",
    "\n",
    "\n",
    "# skipValidityCheck(postings_linkedLists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Processing\n",
    "\n",
    "# Read input data\n",
    "input_queries = pd.read_csv(\"queries.txt\", header=None)\n",
    "# Renaming columns\n",
    "input_queries = input_queries.rename(columns={0: \"query\"})\n",
    "\n",
    "input_queries = input_queries.append([{'query':'recent year'}], ignore_index=True)\n",
    "print(input_queries)\n",
    "\n",
    "input = input_queries.copy()\n",
    "input_queries_processed = []\n",
    "for i in range(len(input)):\n",
    "    input_queries_processed.append(input['query'][i])\n",
    "\n",
    "print(input_queries_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Pre-Processing\n",
    "def processingQueries(queries):\n",
    "    # Pre Processing\n",
    "    # 1. Removed special characters\n",
    "    # 2. Removed excess white spaces\n",
    "    # 3. Whitespace tokenization\n",
    "    # 4. Removed stopwords\n",
    "    # 5. Performed Porter Stemming\n",
    "    query_map = {}\n",
    "    output = []\n",
    "    for i in range(len(queries)):\n",
    "        removed_text = re.sub('[^a-zA-Z.\\d\\s]', ' ', queries[i])\n",
    "        removing_extra_spaces = re.sub('\\s{2,}', ' ', removed_text)\n",
    "        stripped = removing_extra_spaces.strip()\n",
    "        tokens = re.split('\\s+', stripped)\n",
    "        removed_stopwords = removeStopwords(tokens)\n",
    "        stemmed_tokens = stemming(removed_stopwords)\n",
    "        output.append(stemmed_tokens)\n",
    "        query_map[queries[i]] = stemmed_tokens\n",
    "\n",
    "\n",
    "    return output,query_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-at-a-time AND query without skip pointers\n",
    "def daatAnd(input_queries_processed, query_map, postings_linkedLists):\n",
    "\n",
    "\n",
    "    postings_linkedLists = postings_linkedLists.copy()\n",
    "\n",
    "\n",
    "    # print(postings_linkedLists['recent'])\n",
    "\n",
    "    output_postingList = {'postingsList' : {}}\n",
    "    output_json_daatAnd= {'daatAnd': {}, 'daatAndTfIdf':{}}\n",
    "    for i in range(len(input_queries_processed)):\n",
    "        queries_posting_list = {}\n",
    "        for j in input_queries_processed[i]:\n",
    "            if j in postings_linkedLists:\n",
    "                queries_posting_list[j] = postings_linkedLists[j]\n",
    "                copy = copy_list(queries_posting_list[j])\n",
    "                lists = []\n",
    "                while copy != None:\n",
    "                    lists.append(copy.data)\n",
    "                    copy = copy.next\n",
    "\n",
    "                lists_f = [*set(lists)]\n",
    "                output_postingList['postingsList'][j] = sorted(lists_f)\n",
    "            else:\n",
    "                queries_posting_list[j] = None\n",
    "\n",
    "        \n",
    "        for k in queries_posting_list:\n",
    "            queries_posting_list[k] = removeDuplicates(queries_posting_list[k])\n",
    "            \n",
    "        ans, comparisions = merge_lists(queries_posting_list, False)\n",
    "\n",
    "        hashmap = {}\n",
    "\n",
    "        cur = copy_list(ans)\n",
    "        while cur != None:\n",
    "            # print(cur.tf_idf)\n",
    "            hashmap[cur.data] = cur.tf_idf\n",
    "            cur = cur.next\n",
    "\n",
    "        # print(hashmap)\n",
    "        sorted_x = sorted(hashmap.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        # print(sorted_x)\n",
    "\n",
    "        tf_idfDocs = []\n",
    "\n",
    "        for i in sorted_x:\n",
    "            tf_idfDocs.append(i[0])\n",
    "\n",
    "        # print(tf_idfDocs)\n",
    "\n",
    "            \n",
    "\n",
    "        docs = []\n",
    "        while ans != None:\n",
    "            docs.append(ans.data)\n",
    "            ans = ans.next\n",
    "\n",
    "        for i in query_map:\n",
    "            for k in queries_posting_list:\n",
    "                if k in query_map[i]:\n",
    "                    string_name = i\n",
    "                    break\n",
    "\n",
    "        string_name = string_name.strip()\n",
    "        output_json_daatAnd['daatAnd'][string_name] = {}\n",
    "        output_json_daatAnd['daatAndTfIdf'][string_name] = {}\n",
    "\n",
    "        output_json_daatAnd['daatAnd'][string_name]['num_comparisons'] = comparisions\n",
    "        output_json_daatAnd['daatAnd'][string_name]['num_docs'] = len(docs)\n",
    "        output_json_daatAnd['daatAnd'][string_name]['results'] = sorted(docs)\n",
    "        output_json_daatAnd['daatAndTfIdf'][string_name]['num_comparisons'] = comparisions\n",
    "        output_json_daatAnd['daatAndTfIdf'][string_name]['num_docs'] = len(docs)\n",
    "        output_json_daatAnd['daatAndTfIdf'][string_name]['results'] = tf_idfDocs\n",
    "\n",
    "    return output_json_daatAnd, output_postingList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-at-a-time AND query with skip pointers\n",
    "def daatAndSkip(input_queries_processed, query_map, skip_postings_lists):\n",
    "    output_postingListSkip = {'postingsListSkip' : {}}\n",
    "    output_json_daatAndSkip= {'daatAndSkip': {}, 'daatAndSkipTfIdf': {}}\n",
    "    for i in range(len(input_queries_processed)):\n",
    "        queries_posting_listSkip = {}\n",
    "        for j in input_queries_processed[i]:\n",
    "            if j in skip_postings_lists:\n",
    "                queries_posting_listSkip[j] = skip_postings_lists[j]\n",
    "                head = queries_posting_listSkip[j]\n",
    "                lists = []\n",
    "                while head != None:\n",
    "                    if head.skip:\n",
    "                        lists.append(head.data)\n",
    "                    head = head.next\n",
    "\n",
    "                lists_f = [*set(lists)]\n",
    "                output_postingListSkip['postingsListSkip'][j] = sorted(lists_f)\n",
    "            else:\n",
    "                output_postingListSkip[j] = None\n",
    "\n",
    "            \n",
    "        if len(queries_posting_listSkip) >= 2:\n",
    "            for k in queries_posting_listSkip:\n",
    "                queries_posting_listSkip[k] = removeDuplicates(queries_posting_listSkip[k])\n",
    "\n",
    "            \n",
    "            for i in query_map:\n",
    "                for k in queries_posting_listSkip:\n",
    "                    if k in query_map[i]:\n",
    "                        string_name = i\n",
    "                        break\n",
    "\n",
    "            ans, comparisions = merge_lists(queries_posting_listSkip, True)\n",
    "        \n",
    "        elif len(queries_posting_listSkip) == 1:\n",
    "            for key in queries_posting_listSkip:\n",
    "                ans = queries_posting_listSkip[key]\n",
    "                comparisions = 0\n",
    "\n",
    "            for i in query_map:\n",
    "                for k in queries_posting_listSkip:\n",
    "                    if k in query_map[i]:\n",
    "                        string_name = i\n",
    "                        break\n",
    "        else:\n",
    "            ans = None\n",
    "            comparisions = 0\n",
    "            string_name = ''\n",
    "            for i in query_map:\n",
    "                for k in queries_posting_listSkip:\n",
    "                    if k in query_map[i]:\n",
    "                        string_name = i\n",
    "                        break\n",
    "\n",
    "        hashmap = {}\n",
    "\n",
    "        cur = copy_list(ans)\n",
    "        while cur != None:\n",
    "            hashmap[cur.data] = cur.tf_idf\n",
    "            cur = cur.next\n",
    "\n",
    "        sorted_x = sorted(hashmap.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "        tf_idfDocs = []\n",
    "\n",
    "        for i in sorted_x:\n",
    "            tf_idfDocs.append(i[0])\n",
    "\n",
    "        # print(tf_idfDocs)\n",
    "\n",
    "        docs = []\n",
    "        while ans != None:\n",
    "            docs.append(ans.data)\n",
    "            ans = ans.next\n",
    "\n",
    "        string_name = string_name.strip()\n",
    "        output_json_daatAndSkip['daatAndSkip'][string_name] = {}\n",
    "        output_json_daatAndSkip['daatAndSkipTfIdf'][string_name] = {}\n",
    "        output_json_daatAndSkip['daatAndSkip'][string_name]['num_comparisons'] = comparisions\n",
    "        output_json_daatAndSkip['daatAndSkip'][string_name]['num_docs'] = len(docs)\n",
    "        output_json_daatAndSkip['daatAndSkip'][string_name]['results'] = sorted(docs)\n",
    "        output_json_daatAndSkip['daatAndSkipTfIdf'][string_name]['num_comparisons'] = comparisions\n",
    "        output_json_daatAndSkip['daatAndSkipTfIdf'][string_name]['num_docs'] = len(docs)\n",
    "        output_json_daatAndSkip['daatAndSkipTfIdf'][string_name]['results'] = tf_idfDocs\n",
    "\n",
    "    return output_json_daatAndSkip, output_postingListSkip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_corpus = readInputCorpus('input_corpus.txt') #Reading Corpus\n",
    "# input_corpus = convertLower(input_corpus) #LowerCase\n",
    "# input_corpus_prosessed = inputCorpusPreprocessing(input_corpus) #Data Pre-processing\n",
    "# postings_lists = CreateArrayPostingsLists(input_corpus_prosessed) #Array Postings Lists\n",
    "# postings_linkedLists = CovertListToLinkedList(postings_lists, input_corpus_prosessed) #Converts Array Lists to Linked Lists\n",
    "# # # InputCourpusValidityChecks(postings_linkedLists, postings_lists) #Validity Checks\n",
    "# skip_postings_lists = SkipPostingsLists(postings_linkedLists) #Skip Postings List\n",
    "# # # # skipValidityCheck(postings_linkedLists) #Validity Checks\n",
    "# queries = ['the novel coronavirus', 'from an epidemic to a pandemic', 'is hydroxychloroquine effective?', 'recent year']\n",
    "# # # # queries = [\"hello world\", \"hello swimming\", \"swimming going\", \"random swimming\", \"recent year\"]\n",
    "# processed_queries, query_map = processingQueries(queries) #Query Processing\n",
    "\n",
    "# output_dict = {'postingsList': {},\n",
    "#                     'postingsListSkip': {},\n",
    "#                     'daatAnd': {},\n",
    "#                     'daatAndSkip': {},\n",
    "#                     'daatAndTfIdf': {},\n",
    "#                     'daatAndSkipTfIdf': {}}\n",
    "\n",
    "# output_json_daatAnd, output_postingList = daatAnd(processed_queries,query_map, postings_linkedLists)\n",
    "\n",
    "# output_json_daatAndSkip, output_postingListSkip = daatAndSkip(processed_queries, query_map, skip_postings_lists)\n",
    "\n",
    "# output_dict['postingsList'] = output_postingList['postingsList']\n",
    "# output_dict['postingsListSkip'] = output_postingListSkip['postingsListSkip']\n",
    "# output_dict['daatAnd'] = output_json_daatAnd['daatAnd']\n",
    "# output_dict['daatAndSkip'] = output_json_daatAndSkip['daatAndSkip']\n",
    "# output_dict['daatAndTfIdf'] = output_json_daatAnd['daatAndTfIdf']\n",
    "# output_dict['daatAndSkipTfIdf'] = output_json_daatAndSkip['daatAndSkipTfIdf']\n",
    "# print(output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask App\n",
    "@app.route(\"/execute_query\", methods=['POST'])\n",
    "def execute_query():\n",
    "    \n",
    "    queries = request.json[\"queries\"]\n",
    "    \n",
    "    input_corpus = readInputCorpus('input_corpus.txt') #Reading Corpus\n",
    "    input_corpus = convertLower(input_corpus) #LowerCase\n",
    "    input_corpus_prosessed = inputCorpusPreprocessing(input_corpus) #Data Pre-processing\n",
    "    postings_lists = CreateArrayPostingsLists(input_corpus_prosessed) #Array Postings Lists\n",
    "    postings_linkedLists = CovertListToLinkedList(postings_lists, input_corpus_prosessed) #Converts Array Lists to Linked Lists\n",
    "    # # InputCourpusValidityChecks(postings_linkedLists, postings_lists) #Validity Checks\n",
    "    skip_postings_lists = SkipPostingsLists(postings_linkedLists) #Skip Postings List\n",
    "    # # # skipValidityCheck(postings_linkedLists) #Validity Checks\n",
    "    # queries = ['the novel coronavirus', 'from an epidemic to a pandemic', 'is hydroxychloroquine effective?', 'recent year']\n",
    "    # # # queries = [\"hello world\", \"hello swimming\", \"swimming going\", \"random swimming\", \"recent year\"]\n",
    "    processed_queries, query_map = processingQueries(queries) #Query Processing\n",
    "\n",
    "    output_dict = {'postingsList': {},\n",
    "                        'postingsListSkip': {},\n",
    "                        'daatAnd': {},\n",
    "                        'daatAndSkip': {},\n",
    "                        'daatAndTfIdf': {},\n",
    "                        'daatAndSkipTfIdf': {}}\n",
    "\n",
    "    output_json_daatAnd, output_postingList = daatAnd(processed_queries,query_map, postings_linkedLists)\n",
    "\n",
    "    output_json_daatAndSkip, output_postingListSkip = daatAndSkip(processed_queries, query_map, skip_postings_lists)\n",
    "\n",
    "    output_dict['postingsList'] = output_postingList['postingsList']\n",
    "    output_dict['postingsListSkip'] = output_postingListSkip['postingsListSkip']\n",
    "    output_dict['daatAnd'] = output_json_daatAnd['daatAnd']\n",
    "    output_dict['daatAndSkip'] = output_json_daatAndSkip['daatAndSkip']\n",
    "    output_dict['daatAndTfIdf'] = output_json_daatAnd['daatAndTfIdf']\n",
    "    output_dict['daatAndSkipTfIdf'] = output_json_daatAndSkip['daatAndSkipTfIdf']\n",
    "    print(output_dict)\n",
    "\n",
    "\n",
    "    OutJson = ast.literal_eval(str(output_dict))\n",
    "\n",
    "    return flask.jsonify(OutJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\t\"ip\":\"34.125.55.39\",\n",
    "\t\"port\": \"9999\",\n",
    "\t\"name\": \"/execute_query\",\n",
    "\t\"ubit_name\":\"chettisa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip': '34.125.55.39',\n",
       " 'port': '9999',\n",
       " 'name': '/execute_query',\n",
       " 'ubit_name': 'chettisa'}"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"project2_index_details.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# f = open('project2_index_details.json')\n",
    "# data = json.load(f)\n",
    "import pickle\n",
    "\n",
    "with open('project2_index_details.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('project2_index_details.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    \n",
    "print(data==b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
